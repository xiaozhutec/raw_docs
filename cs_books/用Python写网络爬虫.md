#### 《用Python写网络爬虫》


###### 链接: https://pan.baidu.com/s/1RfU0ec096nUmIiRWM_D1SA 提取码: wm92

《用Python写网络爬虫（第 2版》讲解了如何使用Python来编写网络爬虫程序，内容包括网络爬虫简介，从页面中抓取数据的3种方法，提取缓存中的数据，使用多个线程和进程进行并发抓取，抓取动态页面中的内容，与表单进行交互，处理页面中的验证码问题，以及使用Scarpy和Portia进行数据抓取，并在最后介绍了使用本书讲解的数据抓取技术对几个真实的网站进行抓取的实例，旨在帮助读者活学活用书中介绍的技术。



《用Python写网络爬虫（第 2版》适合有一定Python编程经验而且对爬虫技术感兴趣的读者阅读。





![](https://img2020.cnblogs.com/blog/2193560/202101/2193560-20210104172000108-300841681.png)



```
目录
第 1章　网络爬虫简介 1



1.1　网络爬虫何时有用　1



1.2　网络爬虫是否合法　2



1.3　Python 3　3



1.4　背景调研　4



1.4.1　检查robots.txt　4



1.4.2　检查网站地图　5



1.4.3　估算网站大小　6



1.4.4　识别网站所用技术　7



1.4.5　寻找网站所有者　9



1.5　编写第 一个网络爬虫　11



1.5.1　抓取与爬取的对比　11



1.5.2　下载网页　12



1.5.3　网站地图爬虫　15



1.5.4　ID遍历爬虫　17



1.5.5　链接爬虫　19



1.5.6　使用requests库　28



1.6　本章小结　30



第　2章 数据抓取　31



2.1　分析网页　32



2.2　3种网页抓取方法　34



2.2.1　正则表达式　35



2.2.2　Beautiful Soup　37



2.2.3　Lxml　39



2.3　CSS选择器和浏览器控制台　41



2.4　XPath选择器　43



2.5　LXML和家族树　46



2.6　性能对比　47



2.7　抓取结果　49



2.7.1　抓取总结　50



2.7.2　为链接爬虫添加抓取回调　51



2.8　本章小结　55



第3章　下载缓存　56



3.1　何时使用缓存　57



3.2　为链接爬虫添加缓存支持　57



3.3　磁盘缓存　60



3.3.1　实现磁盘缓存　62



3.3.2　缓存测试　64



3.3.3　节省磁盘空间　65



3.3.4　清理过期数据　66



3.3.5　磁盘缓存缺点　68



3.4　键值对存储缓存　69



3.4.1　键值对存储是什么　69



3.4.2　安装Redis　70



3.4.3　Redis概述　71



3.4.4　Redis缓存实现　72



3.4.5　压缩　74



3.4.6　测试缓存　75



3.4.7　探索requests-cache　76



3.5　本章小结　78



第4章　并发下载　79



4.1　100万个网页　79



4.2　串行爬虫　82



4.3　多线程爬虫　83



4.4　线程和进程如何工作　83



4.4.1　实现多线程爬虫　84



4.4.2　多进程爬虫　87



4.5　性能　91



4.6　本章小结　94



第5章　动态内容　95



5.1　动态网页示例　95



5.2　对动态网页进行逆向工程　98



5.3　渲染动态网页　104



5.3.1　PyQt还是PySide　105



5.3.2　执行JavaScript　106



5.3.3　使用WebKit与网站交互　108



5.4　渲染类　111



5.5　本章小结　117



第6章　表单交互　119



6.1　登录表单　120



6.2　支持内容更新的登录脚本扩展　128



6.3　使用Selenium实现自动化表单处理　132



6.4　本章小结　135



第7章　验证码处理　136



7.1　注册账号　137



7.2　光学字符识别　140



7.3　处理复杂验证码　144



7.4　使用验证码处理服务　144



7.4.1　9kw入门　145



7.4.2　报告错误　150



7.4.3　与注册功能集成　151



7.5　验证码与机器学习　153



7.6　本章小结　153



第8章　Scrapy　154



8.1　安装Scrapy　154



8.2　启动项目　155



8.2.1　定义模型　156



8.2.2　创建爬虫　157



8.3　不同的爬虫类型　162



8.4　使用shell命令抓取　163



8.4.1　检查结果　165



8.4.2　中断与恢复爬虫　167



8.5　使用Portia编写可视化爬虫　170



8.5.1　安装　170



8.5.2　标注　172



8.5.3　运行爬虫　176



8.5.4　检查结果　176



8.6　使用Scrapely实现自动化抓取　177



8.7　本章小结　178



第9章　综合应用　179



9.1　Google搜索引擎　179



9.2　Facebook　184



9.2.1　网站　184



9.2.2　Facebook API　186



9.3　Gap　188



9.4　宝马　192



9.5　本章小结　196
```



***

最后，这里为大家准备了几百本的互联网电子书，有需要的过来取吧。[点击获取](https://mp.weixin.qq.com/s/dFqVQ2qJxvQ0YrIlPISJuw)

*本页书籍均来自网络，如有侵权，请联系我立即删除。我的邮箱：yaojianguolq@163.com*




